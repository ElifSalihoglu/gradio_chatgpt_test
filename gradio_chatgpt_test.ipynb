{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ElifSalihoglu/gradio_chatgpt_test/blob/main/gradio_chatgpt_test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install Requirement"
      ],
      "metadata": {
        "id": "0l3MD3312Itl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# !pip uninstall Pillow\n",
        "# !pip install --upgrade Pillow\n",
        "# import PIL\n",
        "# print(PIL.__version__)"
      ],
      "metadata": {
        "id": "zxvS6Afa_RI7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install requests==2.27.1\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6qVzm1f2s6_U",
        "outputId": "573fe683-f1ef-4a5d-a850-854104c44f7c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting requests==2.27.1\n",
            "  Downloading requests-2.27.1-py2.py3-none-any.whl (63 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/63.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━\u001b[0m \u001b[32m61.4/63.1 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests==2.27.1) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests==2.27.1) (2023.7.22)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests==2.27.1) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests==2.27.1) (3.4)\n",
            "Installing collected packages: requests\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.31.0\n",
            "    Uninstalling requests-2.31.0:\n",
            "      Successfully uninstalled requests-2.31.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "chromadb 0.4.5 requires requests>=2.28, but you have requests 2.27.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed requests-2.27.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6WI5qfkUe0Q0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc8fdf64-8c5f-48a6-fad4-7164d0b3a7f7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.27.1, but you have requests 2.31.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install openai -q\n",
        "!pip install langchain -q\n",
        "!pip install chromadb -q\n",
        "!pip install tiktoken -q\n",
        "!pip install pypdf -q\n",
        "!pip install unstructured[local-inference] -q\n",
        "!pip install gradio -q\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import Libraries"
      ],
      "metadata": {
        "id": "TgkUctQX2Pbg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aUTbnXkHe6Rv"
      },
      "outputs": [],
      "source": [
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "\n",
        "import os\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"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\"\n",
        "\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "llm = ChatOpenAI(temperature=0,model_name=\"gpt-3.5-turbo\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import openai\n",
        "\n",
        "openai.api_key = \"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\"\n",
        "openai.Model.list()"
      ],
      "metadata": {
        "id": "4hsenKf933EE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Da3yPddNjxY"
      },
      "source": [
        "# Load your data into Reports folder"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install poppler-utils\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qoir5Y5xvU_g",
        "outputId": "a7c8746d-82bb-4319-d336-8b137de8a071"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following NEW packages will be installed:\n",
            "  poppler-utils\n",
            "0 upgraded, 1 newly installed, 0 to remove and 16 not upgraded.\n",
            "Need to get 186 kB of archives.\n",
            "After this operation, 696 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 poppler-utils amd64 22.02.0-2ubuntu0.2 [186 kB]\n",
            "Fetched 186 kB in 1s (344 kB/s)\n",
            "Selecting previously unselected package poppler-utils.\n",
            "(Reading database ... 120511 files and directories currently installed.)\n",
            "Preparing to unpack .../poppler-utils_22.02.0-2ubuntu0.2_amd64.deb ...\n",
            "Unpacking poppler-utils (22.02.0-2ubuntu0.2) ...\n",
            "Setting up poppler-utils (22.02.0-2ubuntu0.2) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pytesseract\n",
        "!apt-get install tesseract-ocr"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iZPUMNCpv9FO",
        "outputId": "0a02cee5-7add-4921-9449-205efcc4dc11"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pytesseract in /usr/local/lib/python3.10/dist-packages (0.3.10)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.10/dist-packages (from pytesseract) (23.1)\n",
            "Requirement already satisfied: Pillow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from pytesseract) (9.4.0)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  tesseract-ocr-eng tesseract-ocr-osd\n",
            "The following NEW packages will be installed:\n",
            "  tesseract-ocr tesseract-ocr-eng tesseract-ocr-osd\n",
            "0 upgraded, 3 newly installed, 0 to remove and 16 not upgraded.\n",
            "Need to get 4,816 kB of archives.\n",
            "After this operation, 15.6 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-eng all 1:4.00~git30-7274cfa-1.1 [1,591 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-osd all 1:4.00~git30-7274cfa-1.1 [2,990 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr amd64 4.1.1-2.1build1 [236 kB]\n",
            "Fetched 4,816 kB in 1s (6,095 kB/s)\n",
            "Selecting previously unselected package tesseract-ocr-eng.\n",
            "(Reading database ... 120541 files and directories currently installed.)\n",
            "Preparing to unpack .../tesseract-ocr-eng_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-eng (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-osd.\n",
            "Preparing to unpack .../tesseract-ocr-osd_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-osd (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr.\n",
            "Preparing to unpack .../tesseract-ocr_4.1.1-2.1build1_amd64.deb ...\n",
            "Unpacking tesseract-ocr (4.1.1-2.1build1) ...\n",
            "Setting up tesseract-ocr-eng (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-osd (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr (4.1.1-2.1build1) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vSxialfUTh9b",
        "outputId": "3b577510-3a0a-4847-93a7-87398f49273d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of documents: 2\n"
          ]
        }
      ],
      "source": [
        "from langchain.document_loaders import DirectoryLoader\n",
        "from langchain.document_loaders.csv_loader import CSVLoader\n",
        "\n",
        "pdf_loader = DirectoryLoader('./Reports/', glob=\"**/*.pdf\")\n",
        "txt_loader = DirectoryLoader('./Reports/', glob=\"**/*.txt\")\n",
        "word_loader = DirectoryLoader('./Reports/', glob=\"**/*.docx\")\n",
        "csv_loader = DirectoryLoader('./Reports/', glob=\"**/*.csv\")\n",
        "\n",
        "loaders = [pdf_loader, txt_loader, word_loader, csv_loader]\n",
        "documents = []\n",
        "for loader in loaders:\n",
        "    documents.extend(loader.load())\n",
        "\n",
        "print(f\"Total number of documents: {len(documents)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "34HYlsekNona"
      },
      "source": [
        "# Chunk the data, turn into Embeddings and save to VectorStore"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5aUybd6KhN7F"
      },
      "outputs": [],
      "source": [
        "text_splitter = CharacterTextSplitter(chunk_size=2000, chunk_overlap=200)\n",
        "documents = text_splitter.split_documents(documents)\n",
        "\n",
        "embeddings = OpenAIEmbeddings()\n",
        "vectorstore = Chroma.from_documents(documents, embeddings)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ptdb1wVuNzom"
      },
      "source": [
        "# Calling the Langchain's QA chain with Chat History"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tEBWBa1nhQwV"
      },
      "outputs": [],
      "source": [
        "qa = ConversationalRetrievalChain.from_llm(llm, vectorstore.as_retriever(), return_source_documents=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4okS40zMrLIL"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tKmAxuY9rqxK"
      },
      "source": [
        "# Gradio Chat UI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rCLDGbUI5eIz"
      },
      "outputs": [],
      "source": [
        "import gradio as gr\n",
        "# Define chat interface\n",
        "with gr.Blocks() as demo:\n",
        "    chatbot = gr.Chatbot()\n",
        "    msg = gr.Textbox()\n",
        "    clear = gr.Button(\"Clear\")\n",
        "    chat_history = []\n",
        "\n",
        "    def user(query, chat_history):\n",
        "        print(\"User query:\", query)\n",
        "        print(\"Chat history:\", chat_history)\n",
        "\n",
        "        # Convert chat history to list of tuples\n",
        "        chat_history_tuples = []\n",
        "        for message in chat_history:\n",
        "            chat_history_tuples.append((message[0], message[1]))\n",
        "\n",
        "        # Get result from QA chain\n",
        "        result = qa({\"question\": query, \"chat_history\": chat_history_tuples})\n",
        "\n",
        "        # Append user message and response to chat history\n",
        "        chat_history.append((query, result[\"answer\"]))\n",
        "        print(\"Updated chat history:\", chat_history)\n",
        "\n",
        "        return gr.update(value=\"\"), chat_history\n",
        "\n",
        "\n",
        "    msg.submit(user, [msg, chatbot], [msg, chatbot], queue=False)\n",
        "    clear.click(lambda: None, None, chatbot, queue=False)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    demo.launch(debug=True)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}